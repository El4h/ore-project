<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="How Exposure to Diverse Faces Shapes the Computational Mechanism of Face Perception"/>
  <meta property="og:description" content="Poster for CCN 2024"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>How Exposure to Diverse Faces Shapes the Computational Mechanism of Face Perception</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">How Exposure to Diverse Faces Shapes the Computational Mechanism of Face Perception</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://el4h.github.io" target="_blank">Elaheh Akbarifathkouhi</a><sup>1,2</sup>,</span>
                <span class="author-block">
                  <a href="https://www.katharinadobs.com/" target="_blank">Katharina Dobs</a><sup>1,2</sup>,</span>
                  </div>
                  <div class="is-size-5 publication-authors">
                    <span class="author-block">1 Department of Psychology, Justus Liebig University Giessen
                      Giessen, 35394, Germany
                      <br>2 Center for Mind, Brain and Behavior, Universities of Marburg, Giessen, and Darmstadt Marburg, 35032, Germany.
                      </span>
                      <span></span><a href="https://www.uni-giessen.de/en/faculties/f06/psy/welcome-to-the-department-of-psychology?set_language=en" target="_blank">Department of Psychology, Justus Liebig University of Giessen</a></span>
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>-->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="#poster-section" class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Poster</span>
                      </a>
                    </span>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div style="display: flex; justify-content: center; align-items: center;">
        <img src="static/videos/intro.gif" autoplay loop height="150%" muted>
      </div>
      <h2 class="subtitle has-text-centered">
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The Other-Race Effect (ORE) refers to the difficulty humans experience when recognizing faces from races less familiar to them. 
            Prior research has linked the ORE with limited exposure to diverse faces, yet the precise nature of this relationship remains unclear. 
            Here, we use deep convolutional neural networks (CNNs) to investigate how racially varied exposure affects face perception. 
            We trained three CNNs: one on white faces, another on Asian faces, and a Dual CNN on both. While the single- trained CNNs exhibited an 
            ORE on the untrained race, the Dual CNN showed less bias and performed well across both races. 
            Surprisingly, in a target- matching task, the Dual CNN most closely matched both white and Asian participants’ 
            choices, despite their own ORE. Furthermore, only the Dual CNN developed a unified representational space for 
            both races. When testing on an unfamiliar third race, the Dual CNN outperformed single-trained models, 
            highlighting its feature space’s generalizability. Our results show that racially diverse exposure not only 
            reduces biases in CNNs but also results in a unified, more generalized representational geometry, 
            thereby offering new insights into how experience may shape the computational mechanisms of face perception.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- Keywords Section -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Keywords</h2>
        <div class="content has-text-justified">
          <p>
            Face Perception, Deep Convolutional Neural Networks, Other-Race Effect, Representational Geometry
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- End paper abstract -->



<!-- Youtube video -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Methods</h2>
      <br>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/cnns.png" alt="MY ALT TEXT" style="display: block; margin-left: auto; margin-right: auto;"/>
        <h2 class="subtitle has-text-centered">
          To assess the impact of racially varied training on CNNs, we trained three CNNs, all based on the VGG16 architecture, for face recognition tasks: one on 1,654 Asian identities (Asian CNN), another on 1,654 white identities (White CNN), and a third combining both sets of identities (Dual CNN).
        </h2>
       </div>
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/ore-ind.png" alt="MY ALT TEXT" class="centered-image"/>
        <h2 class="subtitle has-text-centered">
          We evaluated each CNN’s performance using a target-matching task on a completely novel dataset comprising 80 female identities (40 Asian and 40 white, 5 images each), none of which were included in the training phase. Performance was quantified by the minimum distance (1 – cosine similarity) in activation patterns (fc7 layer) between two matching images and a target image. The ORE index for each single-trained CNN was computed as above.
        </h2>
       </div>
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/lesioning.png" alt="MY ALT TEXT" style="display: block; margin-left: auto; margin-right: auto;"/>
        <h2 class="subtitle has-text-centered">
          We conducted lesion experiments on the last convolutional layer of the Dual CNN in order to investigate the representational space of the CNNs. We ablated individual filters, assessed their impact on recognition performance for each racial task, and ranked them based on their impact.
        </h2>
       </div>
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/behavioral-data.png" alt="MY ALT TEXT" class="centered-image"/>
        <h2 class="subtitle has-text-centered">
          To directly assess the findings’ relevance to human behavior, we examined the consistency between CNNs’ decisions and those of Asian (n=102) and white (n=269) participants, using behavioral responses obtained from the same target-matching task (Dobs et al., 2023). We analyzed the correlation of decision patterns between participants and CNNs on a trial-by-trial basis.
        </h2>
       </div>
      </div>
    </div>
  </div>
</section>

<!-- End youtube video -->


<!-- Youtube video -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Results</h2>
      <br>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/cnns.png" alt="MY ALT TEXT" style="display: block; margin-left: auto; margin-right: auto;"/>
        <h2 class="subtitle has-text-centered">
          To assess the impact of racially varied training on CNNs, we trained three CNNs, all based on the VGG16 architecture, for face recognition tasks: one on 1,654 Asian identities (Asian CNN), another on 1,654 white identities (White CNN), and a third combining both sets of identities (Dual CNN).
        </h2>
       </div>
       <div class="item">
        <!-- Your image here -->
        <div class="flex-center">
          <img src="static/images/ore-ind.png" alt="MY ALT TEXT" class="centered-image"/>
        </div>
        <h2 class="subtitle has-text-centered">
          We evaluated each CNN’s performance using a target-matching task on a completely novel dataset comprising 80 female identities (40 Asian and 40 white, 5 images each), none of which were included in the training phase. Performance was quantified by the minimum distance (1 – cosine similarity) in activation patterns (fc7 layer) between two matching images and a target image. The ORE index for each single-trained CNN was computed as above.
        </h2>
       </div>
       <div class="item">
        <!-- Your image here -->
        <div class="flex-center">
          <img src="static/images/lesioning.png" alt="MY ALT TEXT" class="centered-image"/>
        </div>
        <h2 class="subtitle has-text-centered">
          We conducted lesion experiments on the last convolutional layer of the Dual CNN in order to investigate the representational space of the CNNs. We ablated individual filters, assessed their impact on recognition performance for each racial task, and ranked them based on their impact.
        </h2>
       </div>
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/behavioral-data.png" alt="MY ALT TEXT" style="display: block; margin-left: auto; margin-right: auto;"/>
        <h2 class="subtitle has-text-centered">
          To directly assess the findings’ relevance to human behavior, we examined the consistency between CNNs’ decisions and those of Asian (n=102) and white (n=269) participants, using behavioral responses obtained from the same target-matching task (Dobs et al., 2023). We analyzed the correlation of decision patterns between participants and CNNs on a trial-by-trial basis.
        </h2>
       </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->







<!-- Paper poster -->
<section class="hero is-small is-light" id="poster-section">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/poster.pdf" width="100%" height="1300">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">References</h2>
      <pre><code>
        Dobs, K., Yuan, J., Martinez, J., & Kanwisher, N. (2023). Behavioral signatures of face perception emerge in deep neural networks optimized for face recognition. Proceedings of the National Academy of Sciences, 120(32), e2220642120.
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
            <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>